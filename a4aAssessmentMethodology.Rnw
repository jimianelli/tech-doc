\documentclass[a4paper,english,10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color} 
\usepackage{float}
\usepackage{longtable}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage{natbib}
\usepackage{authblk}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{babel}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=3cm,bmargin=2cm,lmargin=2cm,rmargin=3cm}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,        % color of internal links
    citecolor=red,         % color of links to bibliography
    filecolor=blue,        % color of file links
    urlcolor=blue          % color of external links
}

% Define some handy formatting
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\pkg}[1]{{\texttt{#1}}}
\newcommand{\class}[1]{{\textit{#1}}}
\newcommand{\R}{{\normalfont\textsf{R }}{}}
\newcommand{\args}[1]{{\texttt{#1}}}
\newcommand{\E}[1]{\text{E}\left[#1\right]}
\newcommand{\Var}[1]{\text{Var}\left[#1\right]}


%----------------------------------------------------------------------------------

\begin{document}
%\SweaveOpts{concordance=TRUE}

\title{Stock assessment and management advice with a4a methods \\ DRAFT}

\author[1]{Ernesto Jardim}
\author[1]{Colin Millar}
\author[1]{Finlay Scott}
\affil[1]{European Commission, Joint Research Centre, IPSC / Maritime Affairs Unit, 21027 Ispra (VA), Italy}
\affil[*]{Corresponding author \href{mailto:ernesto.jardim@jrc.ec.europa.eu}{ernesto.jardim@jrc.ec.europa.eu}}

\maketitle
\tableofcontents
\newpage

\section{Introduction}

\subsection{Background}

(This section is based on Jardim, et.al, 2014\footnote{Jardim,E., Millar,C.P., Mosqueira,I., Scott,F., Osio,G.C., Ferretti,M., Alzorriz,N., Orio,A. 2014. What if stock assessment is as simple as a linear model? The a4a initiative. ICES JMS. \url{http://icesjms.oxfordjournals.org/content/early/2014/04/03/icesjms.fsu050.abstract}
})

The volume and availability of data useful for fisheries stock assessment is continually increasing. Time series of ‘traditional’ sources of information, such as surveys and landings data are not only getting longer, but also cover an increasing number of species.

For example, in Europe the 2009 revision of the Data Collection Regulation (EU, 2008a) has changed the focus of fisheries sampling programmes away from providing data for individual assessment of ‘key’ stocks (i.e. those that are economically important) to documenting fishing trips, thereby shifting the perspective to a large coastal monitoring programme. The result has been that data on growth and reproduction of fish stocks are being collected for more than 300 stocks in waters where the European fleets operate.

Recognizing that the context above required new methodological developments, the European Commission Joint Research Centre (JRC) started its ‘Assessment for All’ Initiative (a4a), with the aim to develop, test, and distribute methods to assess a large numbers of stocks in an operational time frame, and to build the necessary capacity/expertise on stock assessment and advice provision. 

The long-term strategy of a4a is to increase the number of stock assessments by reducing the workload required to run each analysis and by bringing more scientists/analysts into fisheries management advice. The first is achieved by developing a working framework with the methods required to run all the analysis a stock assessment needs, as well as developing methods to deal with recognized bottlenecks, \emph{e.g.} model averaging to deal with model selection. Such an approach should make the model exploration and selection processes easier, as well as decreasing the burden of moving between software platforms. The second can be achieved by making the analysis more intuitive, thereby attracting more experts to join stock assessment teams.

To achieve these objectives, the Initiative identified a series of tasks, which were or are being carried out, namely:
\begin{itemize}
	\item define a moderate data stock;
	\item develop a stock assessment framework;
	\item develop a forecasting algorithm based on MSE;
	\item organize training courses for marine scientists.
\end{itemize}

\subsubsection{The moderate data stock}

The moderate data stock definition was an important step in the Initiative's development. It clearly focused the initiative on stocks with some information, moving away from the data-poor stocks, but without moving into data eager methodologies. It was recognized that there's a lot of research on both extremes of the data availability spectrum, but the middle 'region' is most of the times left behind.

The 'moderate data stock', which constitutes the entry level of our analysis, has at least the following datasets, which can be assembled in different ways, using distinct methods.
 
\begin{itemize}
	\item in relation to exploitation:
	\begin{itemize}
		\item nominal effort (optional, needed in case CPUE indices are to be derived);
		\item volume of catches, which may be split in landings and discards, or not;
		\item length frequencies of the catches, landings or discards;
	\end{itemize}
	\item in relation to biology:
	\begin{itemize}
		\item knife edge maturity ogive;
		\item indication of growth model and parameters;
		\item length-weight relationship;
	\end{itemize}
	\item in relation to abundance:
	\begin{itemize}
		\item index of abundance.
	\end{itemize}	
\end{itemize}

\subsubsection{The stock assessment framework}

The stock assessment model framework is a non-Linear catch-at-age model implemented in R/FLR/ADMB that can be applied rapidly to a wide range of situations with low parametrization requirements. Later we'll come back to these characteristics and it's application.

\subsubsection{MSE}

Regarding the MSE, it's seen as a sophisticated forecasting algorithm that takes into account structural uncertainty about stock dynamics (growth, recruitment, maturity) and on exploitation by commercial fleets (selectivity), embedding the framework of decision making. 

\subsubsection{Training}

During the last 2 years JRC organized 4 courses of introduction to R and FLR: Varese, January 2012; Varese, ??? ...

In 2013 a short course about a4a methods was organized in Lisbon, the first full course on FLR and a4a methods was organized in CEFAS, March 2014 and another one is planned for August 2014.

These courses are open to all participants and don't have an attendance fee.

\subsection{The a4a approach to stock assessment and management advice}

The approach presented here is split in 4 steps: (i) converting length data to age data using a growth model, (ii) modelling natural mortality, (iii) assessing the stock, and (iv) MSE\footnote{Under development, to be released with version 2.0, scheduled for the fourth quarter of 2014}.

These steps may be followed in sequence or independently, depending on the user's preferences. All that is needed is to use the objects provided by the previous step and provide the objects required by the next, so that data flows between steps smoothly. One can make the analogy with a lego build, where for each layer the builder may use the pieces provided by a particular boxset, or make use of pieces from other boxsets. Figure \ref{fig:inout} shows the process including the class of the objects that carry the data (in black).

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{./inout}
\caption{In/out process of the a4a approach. The boxes in black represent the classes of the objects that carry the information for each step and out of each step.}
\label{fig:inout}
\end{figure}

Analysis related with projections and biological reference points are dealt by the FLR packages FLash and FLBRP. As such the Initiative does not provide specific methods for these analysis.

Steps 1 and 2 are simulation based, there's no fitting of growth models or natural mortality models. The rationale is to provide tools that allow the uncertainty associated with these processes to be carried on into the stock assessment. This approach allows the users to pick up the required information from other sources of information, like papers, PhDs, Fishbase, other stocks, etc. Baring in mind that the entry level stock may not have hard information on the processes of growth or natural mortality, but generic information about life history invariants can be used. For example, Nokome Bentley is developing an R package that uses bayesian networks to estimate empirical distributions for these parameters from Fishbase.

\subsection{Loading libraries, data and defining some useful functions}

<<load_libraries, message=FALSE, warning=FALSE>>=
library(FLa4a)
library(XML)
library(reshape2)
library(diagram)
data(ple4)
data(ple4.indices)
@

<<knitr_opts, echo=FALSE>>=
opts_chunk$set(dev='png', cache=TRUE, fig.align='center', background='white',  warning=FALSE, message=FALSE, dev="png", dev.args=list(type="cairo"), dpi=96, highlight=FALSE)
@

<<trans_funcs, message=FALSE, warning=FALSE>>=
# functions for transforming the data

# quant 2 quant
qt2qt <- function(object, id=5, split="-"){
	qt <- object[,id]
	levels(qt) <- unlist(lapply(strsplit(levels(qt), split=split), "[[", 2))
	as.numeric(as.character(qt))
}

# check import and massage
cim <- function(object, n, wt, hrv="missing"){
	v <- object[sample(1:nrow(object), 1),]
	c1 <- c(n[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V6)
	c2 <- c(wt[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V7)
	if(missing(hrv)){
		c1 + c2 == 2	
	} else {
		c3 <- c(hrv[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V8)
		c1 + c2 + c3 == 3	
	}
}

# and a plot for later
plotS4 <- function(object, linktext="typeof", main="S4 class", ...){
	args <- list(...)
	obj <- getClass(as.character(object))
	df0 <- data.frame(names(obj@slots), unlist(lapply(obj@slots, "[[", 1)))
	nms <- c(t(df0))
	nslts <- length(nms)/2
	M <- matrix(nrow = length(nms), ncol = length(nms), byrow = TRUE, data = 0)
	for(i in 1:nslts){
		M[i*2,i*2-1] <- linktext 
	}	
	args$A=M
	args$pos=rep(2, length(nms)/2)
	args$name = nms
	args$main=main
	do.call("plotmat", args)
}

@

\section{Reading files and building FLR objects}

For this document we'll use the plaice in ICES area IV dataset, provided by FLR, and a length-based simulated dataset based on red fish, using Gadget (http://www.hafro.is/gadget), provided by Daniel Howell (Institute of Marine Research, Norway).

In this section we read in the Gadget data files, and transform them into FLR objects.

First we read in the files as data frames and recode some variables.

<<read_gadget_files>>=
# catch
cth.orig <- read.table("data/catch.len", skip=5)

# stock
stk.orig <- read.table("data/red.len", skip=4)

# surveys
idx.orig <- read.table("data/survey.len", skip=5)
idxJmp.orig <- read.table("data/jump.survey.len", skip=5)
idxTrd.orig <- read.table("data/tend.survey.len", skip=5)

# Recode the length categories into something usable

# catch
cth.orig[,5] <- qt2qt(cth.orig)

# stock
stk.orig[,5] <- qt2qt(stk.orig)

# surveys
idx.orig[,5] <- qt2qt(idx.orig)
idxJmp.orig[,5] <- qt2qt(idxJmp.orig)
idxTrd.orig[,5] <- qt2qt(idxTrd.orig)
@

Then we reshape the data frames into six dimensional arrays using cast from package 'reshape2'.

<<cast_data>>=
# catch
cth.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=cth.orig)
cth.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=cth.orig)
hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=cth.orig)

# stock
stk.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=stk.orig)
stk.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=stk.orig)

# surveys
idx.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idx.orig)
idx.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idx.orig)
idx.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idx.orig)
idxJmp.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idxJmp.orig)
idxJmp.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idxJmp.orig)
idxJmp.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idxJmp.orig)
idxTrd.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idxTrd.orig)
idxTrd.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idxTrd.orig)
idxTrd.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idxTrd.orig)
@

We take the arrays and make \class{FLQuant} objects from them.

<<make_FLQs>>=
# catch
dnms <- dimnames(cth.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
cth.n <- FLQuant(cth.n, dimnames=dnms)
cth.wt <- FLQuant(cth.wt, dimnames=dnms)
hrv <- FLQuant(hrv, dimnames=dnms)
units(hrv) <- "f"

# stock
dnms <- dimnames(stk.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
stk.n <- FLQuant(stk.n, dimnames=dnms)
stk.wt <- FLQuant(stk.wt, dimnames=dnms)

# surveys
dnms <- dimnames(idx.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idx.n <- FLQuant(idx.n, dimnames=dnms)
idx.wt <- FLQuant(idx.wt, dimnames=dnms)
idx.hrv <- FLQuant(idx.hrv, dimnames=dnms)

dnms <- dimnames(idxJmp.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idxJmp.n <- FLQuant(idxJmp.n, dimnames=dnms)
idxJmp.wt <- FLQuant(idxJmp.wt, dimnames=dnms)
idxJmp.hrv <- FLQuant(idxJmp.hrv, dimnames=dnms)

dnms <- dimnames(idxTrd.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idxTrd.n <- FLQuant(idxTrd.n, dimnames=dnms)
idxTrd.wt <- FLQuant(idxTrd.wt, dimnames=dnms)
idxTrd.hrv <- FLQuant(idxTrd.hrv, dimnames=dnms)
@

Some sanity checks to check that the resulting objects have matching dimensions.

<<dim_check>>=
# catch
cim(cth.orig, cth.n, cth.wt, hrv)

# stock
cim(stk.orig, stk.n, stk.wt)

# surveys
cim(idx.orig, idx.n, idx.wt, idx.hrv)
cim(idxJmp.orig, idxJmp.n, idxJmp.wt, idxJmp.hrv)
cim(idxTrd.orig, idxTrd.n, idxTrd.wt, idxTrd.hrv)
@

%#------------------------------------------------------------------------------
%# contents
%#------------------------------------------------------------------------------

%# length-weight relationship is a bit odd ...
%xyplot(data~len|year, groups=season, data=cth.wt/cth.n, type="l")

%# length-weight relationship is a bit odd ...
%xyplot(data~len|year, groups=season, data=stk.wt, type="l")

Finally, we make FLR objects from the data.

<<make_FLR_objects>>=
# stock
rfLen.stk <- FLStockLen(stock.n=stk.n, stock.wt=stk.wt, stock=quantSums(stk.wt*stk.n), catch.n=cth.n, catch.wt=cth.wt/cth.n, catch=quantSums(cth.wt), harvest=hrv)
m(rfLen.stk)[] <- 0.05
mat(rfLen.stk)[] <- m.spwn(rfLen.stk)[] <- harvest.spwn(rfLen.stk)[] <- 0
mat(rfLen.stk)[38:59,,,3:4] <- 1

# surveys
rfTrawl.idx <- FLIndex(index=idx.n, catch.n=idx.n, catch.wt=idx.wt, sel.pattern=idx.hrv) 
effort(rfTrawl.idx)[] <- 100

rfTrawlJmp.idx <- FLIndex(index=idxJmp.n, catch.n=idxJmp.n, catch.wt=idxJmp.wt, sel.pattern=idxJmp.hrv) 
effort(rfTrawlJmp.idx)[] <- 100

rfTrawlTrd.idx <- FLIndex(index=idxTrd.n, catch.n=idxTrd.n, catch.wt=idxTrd.wt, sel.pattern=idxTrd.hrv) 
effort(rfTrawlTrd.idx)[] <- 100
@

\pagebreak
\section{Converting length data to age}

The stock assessment framework is based on age dynamics. To use length information it must be pre-processed before used for assessment. The rationale is that the pre-processing should give the analyst the flexibility to use a range of sources of information, \emph{e.g.} literature or online databases, to grab information about the species growth and the uncertainty about the model parameters.

Within the a4a framework this is handled using the \class{a4aGr} class. In this section we introduce the \class{a4aGr} class and look at the variety of ways that parameter uncertainty can be included.

\subsection{a4aGr - The growth class}

The convertion of length data to age is performed through the use of a growth model. The implementation is done through the \class{a4aGr} class.

<<show_a4aGr>>=
showClass("a4aGr")
@

A simple construction of \class{a4aGr} objects requires the model and parameters to be provided.
Check the help file for more information.

Here we show an example using the von Bertalanffy growth model. To create the \class{a4aGr} object it's necessary to pass the model equation ($length \sim time$), the inverse model equation ($time \sim length$) and the parameters. Any growth model can be used as long as it's possible to write the equation in the form explained.

<<a4aGr_vB_example>>=
vbObj <- a4aGr(
	grMod=~linf*(1-exp(-k*(t-t0))), 
	grInvMod=~t0-1/k*log(1-len/linf), 
	params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c("cm","ano-1","ano"))
)

# Check the model and its inverse
lc=20
predict(vbObj, len=lc)
predict(vbObj, t=predict(vbObj, len=lc))==lc
@

The predict method allows the transformation between age and lengths.

<<predict_araGr_example>>=
predict(vbObj, len=5:10+0.5)
predict(vbObj, t=5:10+0.5)
@

\subsection{Adding parameter uncertainty with a multivariate normal distribution}

Uncertainty in the growth model is introduced through the inclusion of parameter uncertainty.
This is done by making use of the parameter variance-covariance matrix (the \code{vcov} slot of the \class{a4aGr} class) and assuming a distribution. The numbers in the variance-covariance matrix could come from the parameter uncertainty from fitting the growth model parameters.

Here we set the variance-covariance matrix by scaling a correlation matrix, using a cv of 0.2.

<<set_vcov_example>>=
# Make an empty cor matrix
cm <- diag(c(1,1,1))
# k and linf are negatively correlated while t0 is independent
cm[1,2] <- cm[2,1] <- -0.5
# scale cor to var using CV=0.2 and some made up values
cv <- 0.2
p <- c(linf=60, k=0.09, t0=-0.01)
vc <- matrix(1, ncol=3, nrow=3)
l <- vc
l[1,] <- l[,1] <- p[1]*cv
k <- vc
k[,2] <- k[2,] <- p[2]*cv
t <- vc
t[3,] <- t[,3] <- p[3]*cv
mm <- t*k*l
diag(mm) <- diag(mm)^2
mm <- mm*cm
# check that we have the correlation foreseen
all.equal(cm, cov2cor(mm))
# Create the a4aGr object as before but now we also include arguments for multivariate normal uncertainty
vbObj <- a4aGr(grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=p["linf"], k=p["k"], t0=p["t0"], units=c("cm","ano-1","ano")), vcov=mm)
@

First we show a simple example where we assume that the parameters are represented using a multivariate normal distribution.
% This covariance matrix can have iterations (i.e. each iteration can have a different covariance matrix). CHECK
% If the parameters or the covariance matrix have iterations then the medians accross iterations are computed before simulating. Check help for \code{mvrnorm} for more information.

<<simulate_vcov_example>>=
# Note that the object we have just created has a single iteration of each parameter
vbObj@params
dim(vbObj@params)
# We simulate from the a4aGr object by calling mvrnorm().
# Here we create 10000 iterations.
# This uses the variance-covariance matrix we created earlier
vbNorm <- mvrnorm(10000,vbObj)
# Now we have 10000 iterations of each parameter, randomly sampled from the multivariate normal distribution
vbNorm@params
dim(vbNorm@params)
@

We can now convert from length to ages data based on the 10000 parameter iterations. This gives us 10000 sets of ages data. For example, here we convert a single length vector: 

<<>>=
ages <- predict(vbNorm, len=5:10+0.5)
dim(ages)
# We show the first ten only as an illustration
ages[,1:10]
@

The marginal distributions can be seen in Figure~\ref{fig:plot_norm_params}.
<<plot_multinorm_marginal, eval=FALSE>>=
par(mfrow=c(3,1))
hist(c(params(vbNorm)["linf",]), main="linf", xlab="")
hist(c(params(vbNorm)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbNorm)["t0",]), main="t0", xlab="")
@

\begin{figure}[h]
<<example_norm_parameter_distributions, echo=FALSE>>=
<<plot_multinorm_marginal>>
@
\caption{The marginal distributions of each of the parameters from using a multivariate normal distribution.}
\label{fig:plot_norm_params}
\end{figure}

The shape of the correlation can be seen in Figure~\ref{fig:plot_norm_scatter}.
<<code_fig_plot_norm_scatter, eval=FALSE>>=
# Plotting scatter plot of growth parameters
splom(data.frame(t(params(vbNorm)@.Data)), pch=".")
@

<<code_plot_mv_growth, eval=FALSE>>=
# Generating and plotting growth curves
boxplot(t(predict(vbNorm, t=0:50+0.5)))
@

\begin{figure}[h]
<<example_norm_parameter_scatter, echo=FALSE>>=
<<code_fig_plot_norm_scatter>>
@
\caption{Scatter plot of the 10000 samples parameter from the multivariate normal distribution.}
\label{fig:plot_norm_scatter}
\end{figure}

Growth curves for the 1000 iterations can be seen in Figure~\ref{fig:plot_mv_growth}.

\begin{figure}[h]
<<example_mv_growth_curve_plot, echo=FALSE>>=
<<code_plot_mv_growth>>
@
\caption{Growth curves using parameters simulated from a multivariate normal distribution.}
\label{fig:plot_mv_growth}
\end{figure}

\subsection{Adding parameter uncertainty with a multivariate triangle distribution}
\label{sec:growth_triangle_cop}

One alternative to using normal distributions, (\url{http://en.wikipedia.org/wiki/Triangle_distribution}). These distributions are parametrized using the minimum, maximum and median values. This can be very attractive if the analyst needs to scrape information from the web or literature and perform some kind of meta-analysis.

Example of setting a triangle distribution with values taken from FishBase.

<<tri_example>>=
# The web address for the growth parameters for redfish (Sebastes norvegicus)
addr <- "http://www.fishbase.org/PopDyn/PopGrowthList.php?ID=501"
# Scrape the data
tab <- try(readHTMLTable(addr))
# Interrogate the data table and get vectors of the values
linf <- as.numeric(as.character(tab$dataTable[,2]))
k <- as.numeric(as.character(tab$dataTable[,4]))
t0 <- as.numeric(as.character(tab$dataTable[,5]))
# Set the min (a), max (b) and median (c) values for the parameter as a list of lists
# Note that t0 has no 'c' (median) value. This makes the distribution symmetrical
triPars <- list(list(a=min(linf), b=max(linf), c=median(linf)),
             list(a=min(k), b=max(k), c=median(k)),
             list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2))
# Simulate 10000 times using mvrtriangle
vbTri <- mvrtriangle(10000, vbObj, paramMargins=triPars)
@

The marginals will reflect the uncertainty on the parameter values that were scraped from FishBase but, as we don't really believe the parameters are multivariate normal we adopted a more relaxed distribution based on a \emph{t} copula with triangle marginals.
The marginal distributions can be seen in Figure~\ref{fig:plot_tri_params} and the shape of the correlation can be seen in Figure~\ref{fig:plot_tri_scatter}.

<<code_plot_tri_params, eval=FALSE>>=
# Plot histogram of the marginals
par(mfrow=c(3,1))
hist(c(params(vbTri)["linf",]), main="linf", xlab="")
hist(c(params(vbTri)["k",]), main="k", prob=TRUE, xlab="")
hist(c(params(vbTri)["t0",]), main="t0", xlab="")
@

<<code_plot_tri_scatter, eval=FALSE>>=
# Scatter plot of the parameter values
splom(data.frame(t(params(vbTri)@.Data)), pch=".")
@

\begin{figure}[h]
<<example_tri_parameter_distributions, echo=FALSE>>=
<<code_plot_tri_params>>
@
\caption{The marginal distributions of each of the parameters from using a multivariate triangle distribution.}
\label{fig:plot_tri_params}
\end{figure}


\begin{figure}[h]
<<example_tri_parameter_scatter, echo=FALSE>>=
<<code_plot_tri_scatter>>
@
\caption{Scatter plot of the 10000 samples parameter from the multivariate triangle distribution.}
\label{fig:plot_tri_scatter}
\end{figure}

We can still use \code{predict()} to get see the growth model uncertainty (Figure~\ref{fig:plot_tri_growth}).

<<code_plot_tri_growth, eval=FALSE>>=
# PLot growth curve
boxplot(t(predict(vbTri, t=0:20+0.5)))
@

\begin{figure}[h]
<<example_tri_parameter_growth, echo=FALSE>>=
<<code_plot_tri_growth>>
@
\caption{Growth curves using parameters simulated from a multivariate triangle distribution.}
\label{fig:plot_tri_growth}
\end{figure}

Remember that the above examples use a variance-covariance matrix that we made up.
If you want to be really geeky, you can scrape the entire growth parameters dataset from FishBase and compute the shape of the variance-covariance matrix yourself.

\subsection{Adding parameter uncertainty with other copulas}

A more general approach to adding parameter uncertainty is to make use of whatever copula and marginal distribution you want.
This is possible with the \code{mvrcop()} function. The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package \emph{copula} for more. 

% Needs more explanation

<<copula_triangle_example>>=
vbCop <- mvrcop(10000, vbObj, copula="archmCopula", family="clayton", param=2, margins="triangle", paramMargins=triPars)
@

The shape of the correlation changes (Figure~\ref{fig:plot_cop_tri_scatter}) as well as the resulting growth curves (Figure~\ref{fig:plot_cop_tri_growth}). 

<<code_plot_cop_tri_scatter, eval=FALSE>>=
# Scatter plot of parameter values
splom(data.frame(t(params(vbCop)@.Data)), pch=".")
@

<<code_plot_cop_tri_growth, eval=FALSE>>=
boxplot(t(predict(vbCop, t=0:20+0.5)))
@

\begin{figure}[h]
<<example_cop_tri_scatter, echo=FALSE>>=
<<code_plot_cop_tri_scatter>>
@
\caption{Scatter plot of the 10000 samples parameter from the using an archmCopula copula with triangle margins.}
\label{fig:plot_cop_tri_scatter}
\end{figure}

\begin{figure}[h]
<<example_cop_tri_growth, echo=FALSE>>=
<<code_plot_cop_tri_growth>>
@
\caption{Growth curves from the using an archmCopula copula with triangle margins.}
\label{fig:plot_cop_tri_growth}
\end{figure}

\subsection{The \code{l2a()} method}

After introducing uncertainty in the growth model through the parameters it's time to transform the length-based dataset into an age-based dataset. The method that deals with this process is \code{l2a()}. The implementation of this method for the \class{FLQuant} class is the main workhorse. There's two other implementations, for the \class{FLStock} and \class{FLIndex} classes, which are mainly wrappers that call the \class{FLQuant} method several times.

When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed. For example, individuals in length classes 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined?

If the values are abundances then the values should be summed. Summing other types of values such as weights does not make sense. Instead these values are averaged over the length classes (weighted by the abundance for weights). Fishing mortality is not computed to avoid making wrong assumptions about the meaning of F at length. This is controlled using the \code{stat} argument which can be either \code{mean} or \code{sum} (the default).

We demonstrate the method by converting a catch-at-length \class{FLQuant} to a catch-at-age \class{FLQuant}. First we make an \class{a4aGr} object with a multivariate triangle distribution. We use 10 iterations as an example. And call \code{l2a} by passing in the length-based \class{FLQuant} and the \class{a4aGr} object.

<<FLQ_l2a, message=FALSE, warning=FALSE>>=
vbTriSmall <- mvrtriangle(10, vbObj, paramMargins=triPars)
cth.n <- l2a(catch.n(rfLen.stk), vbTriSmall)
@

<<example_flq_slice>>=
dim(cth.n)
@

In the previous example, the \class{FLQuant} object that was sliced (\code{catch.n(rfLen.stk)}) had only one iteration. This iteration was sliced by each of the iterations in the growth model. It is possible for the \class{FLQuant} object to have the same number of iterations as the growth model, in which case each iteration of the \class{FLQuant} and the growth model are used together. It is also possible for the growth model to have only one iteration while the \class{FLQuant} object has many iterations. The same growth model is then used for each of the \class{FLQuant} iterations. As with all FLR objects, the general rule is \emph{one or n} iterations.


As well as converting one \class{FLQuant} at a time, we can convert entire \class{FLStock} and \class{FLIndex} objects. In these cases the individual \class{FLQuant} slots of those classes are converted from length-based to age-based. As mentioned above, the aggregation method depends on the type of values the slots contain. The abundance slots (\code{*.n}, such as \code{stock.n}) are summed. The \code{*.wt}, \code{m}, \code{mat}, \code{harvest.spwn} and \code{m.spwn} slots of an \class{FLStock} object are averaged. The \code{index}, \code{catch.wt}, \code{index.var}, \code{sel.pattern} and \code{index.q} slots of an \class{FLIndex} object are averaged\footnote{Still working on l2a for index. Not all of these slots can be averaged}.

The method for \class{FLStock} classes takes an additional argument for the plusgroup.

<<FLS_FLI_l2a, message=FALSE, warning=FALSE>>=
aStk <- l2a(rfLen.stk, vbTriSmall, plusgroup=14)
aIdx <- l2a(rfTrawl.idx, vbTriSmall)
@

When converting with \code{l2a()} all lengths above Linf are converted to the maximum age, as there is no information in the growth model on how to deal with individuals larger than Linf. 

\pagebreak
\section{Natural mortality}

Natural mortality is dealt with as an external parameter to the stock assessment model. The rationale is similar to that of growth: one should be able to grab information from a range of sources and feed it into the assessment.

The mechanism used by a4a is to build an interface that makes it transparent, flexible and hopefully easy to explore different options. In relation to natural mortality it means that the analyst should be able to use distinct models like Gislasson's, Charnov's, Pauly's, etc in a coherent framework making it possible to compare the outcomes of the assessment. 

Within the a4a framework, the general method for inserting natural mortality in the stock assessment is to:

\begin{itemize}
    \item Create an object of class \class{a4aM} which holds the model and parameters to be used to generate the natural mortality.
    \item Add uncertainty to the parameters in the \class{a4aM} object.
    \item Use the \code{m()} method on \class{a4aM} object to create an age or length based \class{FLQuant} object of the required dimensions.
\end{itemize}

The resulting \class{FLQuant} object can then be directly inserted into an \class {FLStock} object to be used for the assessment.   

In this section we go through each of the steps in detail using a variety of different models.

\subsection{\code{a4aM} - The M class}

Natural mortality is implemented in a class named \class{a4aM}. This class is made up of three models of the class \class{FLModelSim}. Each model represents one effect: an age or length effect, a scaling (level) effect and a time trend, named \code{shape}, \code{level} and \code{trend}, respectively. The impact of the models is multiplicative, i.e. the overal natural mortality is given by \class{shape} x  \class{level} x \class{trend}. Check the help files for more information.

<<showClass_a4aM>>=
showClass("a4aM")
@

The \class{a4aM} constructor requires that the models and parameters are provided. The default method will build each of these models as a constant value of 1. For example the usual "0.2" guessestimate could be set up by setting the \code{level} model to have a single parameter with a fixed value, while the other two models, \class{shape} and \class{trend}, have a default value of 1 (effectively, they have no effect).

<<m_02>>=
mod02 <- FLModelSim(model=~a, params=FLPar(a=0.2))
m1 <- a4aM(level=mod02)
m1
@

More interesting natural mortality shapes can be set up using biological knowledge. The following example uses an exponential decay over ages (implying that resulting \class{FLQuant} generated by the \code{m()} method will be age beased). We also use Jensen's second estimator (Kenshington, 2013) as a scaling \code{level} model. This is based on the von Bertalanffy $K$ parameter, $M=1.5K$. 

<<jensen_second_m>>=
shape2 <- FLModelSim(model=~exp(-age-0.5))
level2 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
m2 <- a4aM(shape=shape2, level=level2)
m2
@ 

Note that the \code{shape} model has \code{age} as a parameter of the model but is not set using the \code{params} argument.

The \code{shape} model does not have to be age-based. For example, here we set up a \code{shape} model using Gislaon's second estimator (Kenshington, 2013):
%$M_l=K(\frac{L_\inf}{l})^{1.5}$.
$M_l=K(\frac{L_inf}{l})^1.5$
We use the default \code{level} and \class{trend} models.
% Current m() method is not ideal for length based methods as you cannot specify length range and half-widths to make compatible with FLStockLen

Check a length-base shape model.

<<gis_shape>>=
shape_len <- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=60, K=0.4))
m_len <- a4aM(shape=shape_len)
@

As an alternative, an external factor may impact the natural mortality. This can be added through the \code{trend} model. Suppose M depends on the NAO through some mechanism that results in having lower M when NAO is negative and higher when it's positive. The impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter. 

We use this to make a complicated natural mortality model with an age based shape model, a level model based on $K$ and a trend model driven by NAO, where M increases 50\% if NAO is positive on the first quarter.

<<nao_m>>=
# Get NAO
nao.orig <- read.table("http://www.cdc.noaa.gov/data/correlation/nao.data", skip=1, nrow=62, na.strings="-99.90")
dnms <- list(quant="nao", year=1948:2009, unit="unique", season=1:12, area="unique")
# Build an FLQuant from the NAO data
nao.flq <- FLQuant(unlist(nao.orig[,-1]), dimnames=dnms, units="nao")
# Build covar by calculating mean over the first 3 months
nao <- seasonMeans(nao.flq[,,,1:3]) 
# Turn into Boolean
nao <- (nao>0)
trend3 <- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5))
# Constructor
shape3 <- FLModelSim(model=~exp(-age-0.5))
level3 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
m3 <- a4aM(shape=shape3, level=level3, trend=trend3)
m3
@

\subsection{Adding multivariate normal parameter uncertainty}

Uncertainty on natural mortality is added through uncertainty on the parameters. In the case of the \class{a4aM} class it makes use of the \class{FLModelSim} \code{mvr()} methods. A wrapper for \code{mvrnorm} was implemented, but all the other options must be carried out in each sub-model at the time.

<<mvrnorm_m>>=
shape4 <- FLModelSim(model=~exp(-age-0.5))
level4 <- FLModelSim(model=~k^0.66*t^0.57, params=FLPar(k=0.4, t=10), vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2)))
trend4 <- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5), vcov=matrix(0.02))
m4 <- a4aM(shape=shape4, level=level4, trend=trend4)
m4 <- mvrnorm(100, m4)
m4
# Look at the level model (for example)
m4@level
# Note the variance in the parameters
# The trend model also has uncertainty
params(trend(m4))
# However, the shape model has no parameters and no uncertainty
params(shape(m4))
@ 

In this particular case, the \code{shape} model will not be randomized because it doesn't have a variance covariance matrix. Also note that because there is only one parameter in the \code{trend} model, the randomization will use a univariate normal distribution.

The same model could be achieved using \code{mnrnorm()} on each model component:

<<univariate_m>>=
m4 <- a4aM(shape=shape4, level=mvrnorm(100, level4), trend=mvrnorm(100, trend4))
@

%Note: How to include ageing error ???

\subsection{Adding parameter uncertainty with copulas}

We can also use copulas add parameter uncertainty to the natural mortality model, similar to the way we use them for the growth model \ref{sec:growth_triangle_cop}. As stated above these processes make use of the methods implemented for the \class{FLModelSim} class.

% EXPAND...

In the following example we'll use Gislason's second estimator (REF), $M_l=K(\frac{L_inf}{l})^1.5$ and a triangle copula to model parameter uncertainty in natural mortality.

<<gis_copula>>=
linf <- 60
k <- 0.4
# vcov matrix (make up some values)
mm <- matrix(NA, ncol=2, nrow=2)
# 10% cv
diag(mm) <- c((linf*0.1)^2, (k*0.1)^2)
# 0.2 correlation
mm[upper.tri(mm)] <- mm[lower.tri(mm)] <- c(0.05)
# a good way to check is using cov2cor
cov2cor(mm)
# create object
mgis2 <- FLModelSim(model=~k*(linf/len)^1.5, params=FLPar(linf=linf, k=k), vcov=mm)
# set the lower, upper and (optionally) centre of the parameters
# Without the centre, the triangle is symmetrical
pars <- list(list(a=55,b=65), list(a=0.3, b=0.6, c=0.35))
mgis2 <- mvrtriangle(1000, mgis2, paramMargins=pars)
mgis2
@

The resulting parameter estimates and marginal distributions can be seen in Figure~\ref{fig:plot_tri_gis_m} and \ref{fig:plot_tri_gis_m_hist}

<<code_plot_tri_gis_m, eval=FALSE>>=
splom(t(params(mgis2)@.Data))
@

<<code_plot_tri_gis_m_hist, eval=FALSE>>=
par(mfrow=c(2,1))
hist(c(params(mgis2)["linf",]), main="Linf", xlab="")
hist(c(params(mgis2)["k",]), main="K", xlab="")
@

\begin{figure}[h]
<<example_tri_gis_m_scatter, echo=FALSE>>=
<<code_plot_tri_gis_m>>
@
\caption{Parameter estimates for Gislason's second natural mortality model from using a triangle distribution.}
\label{fig:plot_tri_gis_m}
\end{figure}

\begin{figure}[h]
<<example_tri_gis_m_hist, echo=FALSE>>=
<<code_plot_tri_gis_m_hist>>
@
\caption{Marginal distributions of the parameters for Gislason's second natural mortality model from using a triangle distribution.}
\label{fig:plot_tri_gis_m_hist}
\end{figure}

We now have a new model that can be used for the \code{shape} model. Use the constructor or the set method to add the new model. Note that we have a quite complex method now for \emph{M}. A length based \code{shape} model from Gislason's work, Jensen's third based temperature \code{level} and a time \code{trend} depending on NAO. All of the component models have uncertainty in their parameters.

<<making_complicated_m>>=
m5 <- a4aM(shape=mgis2, level=level4, trend=trend4)
# or
m5 <- m4
shape(m5) <- mgis2
@

\subsection{The "m" method}

Now that we have set up the natural mortality model and added parameter uncertainty, we are ready to generate the \class{FLQuant} of natural mortality. For that we need the \code{m()} method.

The \code{m} method is the workhorse method for computing natural mortality. The method returns an \class{FLQuant} that can be inserted in an \class{FLStock} for usage by the assessment method. Note that if the models use \emph{age} and/or \emph{year} as terms, the method expects these to be included in the call. If they're not, the method will use the range slot to work out the ages and/or years that should be predicted.

% Future developments will also allow for easy insertion into FLStockLen objects.

The size of the \class{FLQuant} object is determined by the \code{min}, \code{max}, \code{minyear} and \code{maxyear} elements of the \code{range} slot of the \class{a4aM} object. By default the values of these elements are set to 0. Giving an \class{FLQuant} with length 1 in the quant and year dimension. The \code{range} slot can be set by hand, or by using the \code{rngquant()} and \code{rngyear()} methods.

The name of the first dimension of the output \class{FLQuant} (e.g. 'age' or 'len') is determined by the parameters of the \code{shape} model. If it is not clear what the name should be then the name is set to 'quant'.

Using the m method to make an FLQuant of constant natural mortality:

<<simple_m>>=
# Start with the simplest model
m1
# Check the range
range(m1)
# Simple - no ages or years
m(m1)
# Set the quant range
rngquant(m1) <- c(0,7) # set the quant range
range(m1)
m(m1)
# Set the year range too
rngyear(m1) <- c(2000, 2010) # set the year range
range(m1)
m(m1)
# Note the name of the first dimension is 'quant'
@

The next example has an age based shape. As the shape model has 'age' as a variable which is not included in the \class{FLPar} slot it is used as the name of the first dimension. Note that in this case \emph{mbar} becames relevant. It's the range of quants (in this case, ages) that is used to compute the mean level. This mean level will match the value given by the \code{level} model.

The \emph{mbar} range can be changed with the \code{rngmbar()} method.

Using the m method to make an FLQuant with age varying natural mortality:

<<m2>>=
# Remind ourselves of the model
m2
# Simple with no ages or years - note that the first dimension is 'age'
m(m2)
# With ages
rngquant(m2) <- c(0,7)
m(m2)
# With ages and years
rngyear(m2) <- c(2000, 2003)
m(m2)
# Note that the level value is:
predict(level(m2))
# Is the same as
m(m2)["0"]
# This is because the mbar range is currently set to "0" and "0"
range(m2)
# The mean natural mortality value over this range is given by the level model
# We can change the mbar range
rngmbar(m2)<- c(0,5)
range(m2)
# This rescales the natural mortality at age:
m(m2)
# Check that the mortality over the mean range is the same as the level model
quantMeans(m(m2)[as.character(0:5)])
@

The next example uses a time trend for the \code{trend} model. We use the \code{m3} model we made earlier. The \code{trend} model for this model has a covariate, 'nao'. This needs to be passed in to the \code{m()} method. The year range of the 'nao' covariate should match that of the \code{range} slot.

Using the m method to make an FLQuant with a time trend:

<<m3_trend>>=
# Simple, pass in a single nao value (only one year)
m(m3, nao=1)
# Set some ages
rngquant(m3) <- c(0,7)
m(m3, nao=0)
# With ages and years - passing in the NAO data as numeric (1,0,1,0)
rngyear(m3) <- c(2000, 2003)
m(m3, nao=as.numeric(nao[,as.character(2000:2003)]))
@

The final example show how \code{m()} can be used to make an \class{FLQuant} with uncertainty (see Figure~\ref{fig:uncertain_m}). We use the \code{m4} from earlier with uncertainty on the level and trend parameters.

Using the m method to make an FLQuant with uncertainty:

<<m4_uncertainty_m>>=
# Simple - no time trend but with iterations
m(m4, nao=1)
dim(m(m4, nao=1))
# With ages
rngquant(m4) <- c(0,7)
m(m4, nao=0)
dim(m(m4, nao=0))
# With ages and years
rngyear(m4) <- c(2000, 2003)
m(m4, nao=as.numeric(nao[,as.character(2000:2003)]))
dim(m(m4, nao=as.numeric(nao[,as.character(2000:2003)])))
@

<<code_uncertain_m, eval=FALSE>>=
bwplot(data~factor(age)|year, data=m(m4, nao=as.numeric(nao[,as.character(2000:2003)])))
@

\begin{figure}[h]
<<plot_uncertain_m, echo=FALSE>>=
<<code_uncertain_m>>
@
\caption{Natural mortality with age and year trend.}
\label{fig:uncertain_m}
\end{figure}

\pagebreak
\section{Running assessments}


In the \code{a4a} assessment model, the model structure is defined by submodels. These are models for the different parts of a statistical catch at age model that requires structural assumptions.

There are effectively 5 submodels in operation: the model for F-at-age, a model for initial age structure, a model for recruitment, a (list) of model(s) for survey catchability-at-age, and a list of models for the observation variance of catch.n and the survey indices. In practice, we fix the variance models and the initial age structure models, but in theory these can be changed.

The submodels form is given using linear models, which opens the possibility of using the linear modelling tools available in R: see for example gam formulas, or factorial design formulas using lm. In R's linear modelling lanquage, a constant model is coded as $\sim 1$, while a slope over age would simply be $\sim age$.  Extending this we can write a traditional year/age separable F model like $\sim factor(age) + factor(year)$.

There are two basic types of assessments available from using \code{a4a}: the management procedure fit and the full assessment fit. The management procedure fit does not compute estimates of covariances and is therefore quicker to execute, while the full assessment fit returns parameter estimates and their covariances at the expense of longer fitting time.

\subsection{Stock assessment model details}

The statistical catch at age model is based on the well known Baranov catch equation. 

\begin{equation*}
e^{\E{\log C}} = \frac{\color{red}F}{{\color{red}F}+M}\left(1 - e^{-{\color{red}F}-M}\right) {\color{red}R}e^{-\sum {\color{red}F} + M}
\end{equation*}

and

\begin{equation*}
e^{\E{\log I}} = {\color{red}Q} {\color{red}R}e^{-\sum {\color{red}F} + M}
\end{equation*}

and

\begin{equation*}
\Var{\log C_{ay}} = {\color{red}\sigma^2_{ay}} \qquad \Var{\log I_{ays}} = {\color{red}\tau^2_{ays}}
\end{equation*}

linear models for
\begin{itemize}
  \item log F
  \item log Q
  \item log observation variances
  \item log initial age structure  
\end{itemize}

Recruitment is modelled as a \bf{fixed variance} random effect with linear models for
\begin{itemize}
  \item log a
  \item log b
\end{itemize}
where relevant.  Models available: Ricker, Beverton Holt, smooth hockeystick, geometric mean

It is not always obvious that stock assessments are often composed of linear models.

For example, the classical separable F assumption is simply that
\begin{align*}
F_{ay} = S_a \times F_y 
\end{align*}
which, in linear modelling parlance is
\begin{align*}
\log F \sim \text{age} + \text{year} 
\end{align*}

The "language" of linear models has been developing within the statistical community for many years:

  \begin{itemize}
  \item 1965 J. A. Nelder, notation for randomized block design
  \item 1973 Wilkinson and Rodgers, symbolic description for factorial designs
  \item 1990 Hastie and Tibshirani, introduced notation for smoothers
  \item 1991 Chambers and Hastie, further developed for use in S
  \end{itemize}

Many modelling software use this language:  Minitab, spss, genstat, SAS, R, S-plus.

A separable model where the level of F is smooth through time

\begin{align*}
  \log F \sim \text{age} + \text{s(year)}
\end{align*}

A separable model where F is smooth over age

\begin{align*}
  \log F \sim \text{s(age)} + \text{year}
\end{align*}

F is smooth over age and year

\begin{align*}
  \log F \sim \text{s(age, year)}
\end{align*}

A basic set of submodels would be

<<>>=
fmodel <- ~ factor(age) + factor(year)
qmodel <- list(~ factor(age)) 
@

\subsection{Quick and dirty}

The default settings of the stock assessment model work reasoably well. It's an area of research that will improve with time.

<<message=TRUE>>=
data(ple4)
data(ple4.indices)
fit <- sca(ple4, ple4.indices)
@

Note that because the survey index for plaice has missing values we get a warning saying that we assume these values are missing at random, and not because the observations were zero.

<<>>=
res <- residuals(fit, ple4, ple4.indices)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(res, main="Residuals")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
bubbles(res)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
qqmath(res)
@

We can inspect the summaries from this fit my adding it to the origional stock object, for example to see the fitted fbar we can do

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
stk <- ple4 + fit
plot(stk, main="Stock summary")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(stk)), drape = TRUE, main="Fishing mortality", screen = list(x = -90, y=-45))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(stock.n(stk)), drape = TRUE, main="Population", screen = list(x = -90, y=-45))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(catch.n(stk)), drape = TRUE, main="Catches")
@

\subsection{Data structures}

The basic model output is contained in the \texttt{a4aFit} class. This object contains only the fitted values.

<<>>=
showClass("a4aFit")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plotS4("a4aFit", main="a4aFit class", lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.3)
@

Fitted values are stored in the \texttt{stock.n}, \texttt{harvest}, \texttt{catch.n} and \texttt{index} slots.  It also contains information carried over from the stock object used to fit the model: the name of the stock in \texttt{name}, any description provided in \texttt{desc} and the age and year range and mean F range in \texttt{range}.  There is also a wall clock that has a breakdown of the time taken o run the model.

The full assessment fit returns an object of \texttt{a4aFitSA} class:

<<>>=
showClass("a4aFitSA")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plotS4("a4aFitSA", main="a4aFitSA class", lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.3)
@

The additional slots in the assessment output is the \texttt{fitSumm} and \texttt{pars} slots which are containers for model summaries and the model parameters.  The \texttt{pars} slot is a class of type \texttt{SCAPars} which is itself composed of sub-classes, designed to contain the information necessary to simulate from the model.

<<>>=
showClass("SCAPars")
showClass("a4aStkParams")
showClass("submodel")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plotS4("SCAPars", main="SCAPars class", lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.3)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plotS4("a4aStkParams", main="a4aStkParams class", lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.3)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plotS4("submodel", main="submodel class", lwd = 1, box.lwd = 2, cex.txt = 0.8, box.size = 0.1, box.type = "square", box.prop = 0.3)
@

for example, all the parameters required so simulate a time-series of mean F trends is contained in the \texttt{stkmodel} slot, which is a class of type \texttt{a4aStkParams}.  This class contains the relevant submodels (see later), their parameters \texttt{params} and the joint covariance matrix \texttt{vcov} for all stock related parameters.

\subsection{The sca method - statistical catch-at-age}

We will now take a look at some examples for F models and the forms that we can get.  Lets start with a separable model in which we model selectivity at age as an (unpenalised) thin plate spline.  We will use the North Sea Plaice data again, and since this has 10 ages we will use a simple rule of thumb that the spline should have fewer than $\frac{10}{2} = 5$ degrees of freedom, and so we opt for 4 degrees of freedom.  We will also do the same for year and model the change in F through time as a smoother with 20 degrees of freedom.

Lets now investigate some variations in the selectivity shape with time, but only a little... we can do this by adding a smooth interaction term in the fmodel

A further move is to free up the Fs to vary more over time

In the last examples the Fs are linked across age and time.  What if we want to free up a specific age class because in the residuals we see a consistent pattern.  This can happen, for example, if the spatial distribution of juvenilles is disconnected to the distribution of adults.  The fishery focuses on the adult fish, and therefore the the F on young fish is a function of the distribution of the juveniles and could deserve a seperate model.  This can be achieved by

Please note that each of these model \emph{structures} lets say, have not been tuned to the data.  The degrees of freedom of each model can be better tuned to the data by using model selection procedures such as AIC or BIC.

\subsubsection{Fishing mortality submodel}

<<>>=
qmodel <- list(~ factor(age)) 
fmodel <- ~ factor(age) + factor(year)
fit <- sca(stock = ple4, indices = ple4.indices[1], fmodel=fmodel, qmodel=qmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
fmodel <- ~ s(age, k=4) + s(year, k = 20)
fit1 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit1)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
fmodel <- ~ s(age, k=4) + s(year, k = 20) + te(age, year, k = c(3,3))
fit2 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit2)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
fmodel <- ~ te(age, year, k = c(4,20))
fit3 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit3)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
fmodel <- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1))
fit4 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit4)), drape = TRUE, screen = list(x = -90, y=-45))
@

\subsubsection{Catchability submodel}

<<>>=
sfrac <- mean(range(ple4.indices[[1]])[c("startf", "endf")])
fmodel <- ~ factor(age) + factor(year)
@

<<>>=
qmodel <- list(~ factor(age)) 
fit <- sca(ple4, ple4.indices[1], fmodel, qmodel)
Z <- (m(ple4) + harvest(fit))*sfrac
lst <- dimnames(fit@index[[1]])
lst$x <- stock.n(fit)*exp(-Z)
stkn <- do.call("trim", lst)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(index(fit)[[1]]/stkn), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
qmodel <- list(~ s(age, k=4))
fit1 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
Z <- (m(ple4) + harvest(fit1))*sfrac
lst <- dimnames(fit1@index[[1]])
lst$x <- stock.n(fit1)*exp(-Z)
stkn <- do.call("trim", lst)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(index(fit1)[[1]]/stkn), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
qmodel <- list(~ te(age, year, k = c(3,40)))
fit2 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
Z <- (m(ple4) + harvest(fit2))*sfrac
lst <- dimnames(fit2@index[[1]])
lst$x <- stock.n(fit2)*exp(-Z)
stkn <- do.call("trim", lst)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(index(fit2)[[1]]/stkn), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
qmodel <- list( ~ s(age, k=4) + year)
fit3 <- sca(ple4, ple4.indices[1], fmodel, qmodel)
Z <- (m(ple4) + harvest(fit3))*sfrac
lst <- dimnames(fit3@index[[1]])
lst$x <- stock.n(fit3)*exp(-Z)
stkn <- do.call("trim", lst)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(index(fit3)[[1]]/stkn), drape = TRUE, screen = list(x = -90, y=-45))
@

\subsubsection{Catchability submodel for age aggregated indices}

Age aggregated indices (biomass, DEPM, etc) have to be passed in the FLIndices object with the name "bio". At the moment only one "bio" index is allowed. Note that in this case the qmodel should be set without age factors. It could have a "year" component though. 

<<>>=
# creating an index
bioidx <- FLIndex(FLQuant(NA, dimnames=list(age="all", year=range(ple4)["minyear"]:range(ple4)["maxyear"])))
index(bioidx) <- stock(ple4)*0.001
index(bioidx) <- index(bioidx)*exp(rnorm(index(bioidx), sd=0.1))
range(bioidx)[c("startf","endf")] <- c(0,0)
# fitting the model
fit <- sca(ple4, FLIndices(bio=bioidx), qmodel=list(~1))
# how is it fiting catchability ? not too well ...
index(fit)[[1]]/(quantSums(stock.n(fit)*stock.wt(ple4)))
@

Residuals are not so good ... one shouldn't expect much from a model with only one biomass index anyway. 


The results are not so bad because most information is coming from the catch at age information.

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(FLStocks(ple4=ple4, fit=ple4+fit))
@

\subsubsection{Stock-recruitment submodel}

<<>>=
fmodel <- ~ s(age, k=4) + s(year, k = 20)
qmodel <- list(~ s(age, k=4))
@

<<>>=
srmodel <- ~ factor(year)
fit <- sca(ple4, ple4.indices[1], fmodel=fmodel, qmodel=qmodel, srmodel=srmodel) 
srmodel <- ~ s(year, k=20)
fit1 <- sca(ple4, ple4.indices[1], fmodel, qmodel, srmodel) 
srmodel <- ~ ricker(CV=0.05)
fit2 <- sca(ple4, ple4.indices[1], fmodel, qmodel, srmodel) 
srmodel <- ~ bevholt(CV=0.05)
fit3 <- sca(ple4, ple4.indices[1], fmodel, qmodel, srmodel) 
srmodel <- ~ hockey(CV=0.05)
fit4 <- sca(ple4, ple4.indices[1], fmodel, qmodel, srmodel) 
srmodel <- ~ geomean(CV=0.05)
fit5 <- sca(ple4, ple4.indices[1], fmodel, qmodel, srmodel) 
flqs <- FLQuants(fac=stock.n(fit)[1], smo=stock.n(fit1)[1], ric=stock.n(fit2)[1], bh=stock.n(fit3)[1], hs=stock.n(fit4)[1], gm=stock.n(fit5)[1])
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year, groups=qname, data=flqs, type="l", main="Recruitment models", auto.key=list(points=FALSE, lines=TRUE, columns=3))
@

\subsection{The a4aSCA method - advanced features}

The default fit is "assessment", which means that the hessian is going to e computed by default.

<<>>=
#fmodel <- ~ s(age, k=4) + s(year, k = 20)
#qmodel <- list( ~ s(age, k=4) + year)
#srmodel <- ~s(year, k=20)
fit <- a4aSCA(ple4, ple4.indices[1]) 
submodels(fit)
@

\subsubsection{N1 model}

<<>>=
n1model <- ~s(age, k=4)
fit1 <- a4aSCA(ple4, ple4.indices[1], n1model=n1model) 
flqs <- FLQuants(smo=stock.n(fit1)[,1], fac=stock.n(fit)[,1])
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~age, groups=qname, data=flqs, type="l", main="N1 models", auto.key=list(points=FALSE, lines=TRUE, columns=2))
@

\subsubsection{Variance model}

One important subject related with fisheries data used for input to stock assessment models is the shape of the variance of the data. It's quite common to have more precision on the most represented ages and less precision on the less frequent ages. Due to the fact that the last do not show so often on the auction markets, on the fishing operations or on survey samples.

By default the model assumes constant variance over time and ages (~ 1 model) but it can use other models specified by the user. This feature requires a call to the a4aInternal method, which gives more options than the a4a method, which in fact is a wrapper.

<<>>=
vmodel <- list(~1, ~1)
fit1 <- a4aSCA(ple4, ple4.indices[1], vmodel=vmodel) 
vmodel <- list(~ s(age, k=4), ~1)
fit2 <- a4aSCA(ple4, ple4.indices[1], vmodel=vmodel) 
flqs <- FLQuants(cts=catch.n(fit1), smo=catch.n(fit2))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year|age, groups=qname, data=flqs, type="l", main="Variance models", scales=list(y=list(relation="free")), auto.key=list(points=FALSE, lines=TRUE, columns=2))
@

\subsubsection{Working with covariates}

<<>>=
nao <- read.table("http://www.cdc.noaa.gov/data/correlation/nao.data", skip=1, nrow=62, na.strings="-99.90")
dnms <- list(quant="nao", year=1948:2009, unit="unique", season=1:12, area="unique")
nao <- FLQuant(unlist(nao[,-1]), dimnames=dnms, units="nao")
nao <- seasonMeans(trim(nao, year=dimnames(stock.n(ple4))$year))
nao <- as.numeric(nao)
@

<<>>=
srmodel <- ~ nao
fit2 <- a4aSCA(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmodel) 
flqs <- FLQuants(fac=stock.n(fit)[1], cvar=stock.n(fit2)[1])
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year, groups=qname, data=flqs, type="l", main="Recruitment model with covariates")
@

<<>>=
srmodel <- ~ ricker(a=~nao, CV=0.1)
fit3 <- a4aSCA(ple4, ple4.indices[1], qmodel=list(~s(age, k=4)), srmodel=srmodel) 
flqs <- FLQuants(fac=stock.n(fit)[1], cvar=stock.n(fit3)[1])
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year, groups=qname, data=flqs, type="l", main="Recruitment model with covariates")
@

\subsubsection{Assessing ADMB files}

To inspect the ADMB files the user must specify the working dir and all files will be left there.

<<>>=
fit1 <- a4aSCA(stk, idx, wkdir="mytest") 
@

\subsection{Predict and simulate}

<<>>=
#fmodel <- ~ s(age, k=4) + s(year, k = 20)
#qmodel <- list( ~ s(age, k=4) + year)
#srmodel <- ~s(year, k=20)
fit <- a4aSCA(ple4, ple4.indices[1]) 
@

\subsubsection{Predict}

<<>>=
fit.pred <- predict(fit)
lapply(fit.pred, names)
@

\subsubsection{simulate}

<<>>=
fits <- simulate(fit, 100)
flqs <- FLQuants(sim=iterMedians(stock.n(fits)), det=stock.n(fit))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year|age, groups=qname, data=flqs, type="l", main="Median simulations VS fit", scales=list(y=list(relation="free")))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
stks <- ple4 + fits
plot(stks)
@

\subsection{Geeky stuff}

<<>>=
#fmodel <- ~ s(age, k=4) + s(year, k = 20)
#qmodel <- list( ~ s(age, k=4) + year)
#srmodel <- ~s(year, k=20)
vmodel <- list(~1, ~1)
fit <- a4aSCA(ple4, ple4.indices[1], vmodel=vmodel) 
@


\subsubsection{External weigthing of likelihood components}

By default the likelihood components are weighted using inverse variance of the parameters estimates. However the user may change this weights by setting the variance of the input parameters, which is done by adding a variance matrix to the catch.n and index.n slots of the stock and index objects. 

<<>>=
stk <- ple4
idx <- ple4.indices[1]
# variance of observed catches
varslt <- catch.n(stk)
varslt[] <- 0.4
catch.n(stk) <- FLQuantDistr(catch.n(stk), varslt)
# variance of observed indices
varslt <- index(idx[[1]])
varslt[] <- 0.1
index.var(idx[[1]]) <- varslt
# run
fit1 <- a4aSCA(stk, idx, vmodel=vmodel) 
flqs <- FLQuants(nowgt=stock.n(fit), extwgt=stock.n(fit1))
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year|age, groups=qname, data=flqs, type="l", main="Likelihood weighting", scales=list(y=list(relation="free")), auto.key=list(points=FALSE, lines=TRUE, columns=2))
@

\subsubsection{More models}

<<>>=
# constant fishing mortality for ages older than 5
fmodel = ~ s(replace(age, age>5, 5), k=4) + s(year, k=20)
fit <- sca(ple4, ple4.indices, fmodel=fmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
# the same model for two periods
fmodel=~s(age, k = 3, by = breakpts(year, 1990))
fit <- sca(ple4, ple4.indices, fmodel=fmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit)), drape = TRUE, screen = list(x = -90, y=-45))
@

<<>>=
# smoother for each age 
fmodel <- ~ factor(age) + s(year, k=10, by = breakpts(age, c(2:8)))
fit <- sca(ple4, ple4.indices, fmodel=fmodel)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
wireframe(data ~ age + year, data = as.data.frame(harvest(fit)), drape = TRUE, screen = list(x = -90, y=-45))
@

\subsubsection{Propagate M uncertainty}


<<>>=
fit <- sca(ple4, ple4.indices)
@

<<>>=
nits <- 25

shape2 <- FLModelSim(model=~exp(-age-0.5))
level4 <- FLModelSim(model=~k^0.66*t^0.57, params = FLPar(k=0.4, t=10), vcov=matrix(c(0.002, 0.01,0.01, 1), ncol=2))
 
trend4 <- FLModelSim(model=~b, params=FLPar(b=0.5), vcov=matrix(0.02))
m4 <- a4aM(shape=shape2, level=level4, trend=trend4)
m4 <- mvrnorm(nits, m4)
range(m4)[] <- range(ple4)[]
range(m4)[c("minmbar","maxmbar")]<-c(1,1)
flq <- m(m4)[]
quant(flq) <- "age"
stk <- propagate(ple4, nits)
m(stk) <- flq

fit1 <- sca(stk, ple4.indices)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(FLStocks(munc=ple4+fit1, m02=ple4+fit))
@

%\subsubsection{Likelihood profiling - ToDo}

%<<>>=
%qk <- 3:8
%fk <- seq(10,50,5)
%liks <- expand.grid(qk=qk, fk=fk)
%liks$loglik <- NA
%for(i in 1:nrow(liks)){
%	ks <- liks[i,]
%	qmodel <- as.formula(substitute(~s(age, k=x) + year, list(x=ks$qk)))
%	fmodel <- as.formula(substitute(~s(age, k=4) + s(year, k=x), list(x=ks$fk)))
%	liks[i,3] <- as.numeric(AIC(sca(ple4, ple4.indices[1], qmodel=list(qmodel), fmodel=fmodel)))
%}
%@

\subsubsection{WCSAM exercise - replicating itself}

<<>>=
fit <- sca(ple4, ple4.indices, fit="assessment")
nits <- 25
stk <- ple4 + fit
fits <- simulate(fit, nits)
stks <- ple4 + fits 
idxs <- ple4.indices[1]
index(idxs[[1]]) <- index(fits)[[1]]
library(parallel)
options(mc.cores=4)
lst <- lapply(split(1:nits, 1:nits), function(x){
	out <- try(sca(iter(stks, x), FLIndices(iter(idxs[[1]], x)))) 
	if(is(out, "try-error")) NULL else out
})

stks2 <- stks
for(i in 1:nits){
	iter(catch.n(stks2), i) <- catch.n(lst[[i]])
	iter(stock.n(stks2), i) <- stock.n(lst[[i]])
	iter(harvest(stks2), i) <- harvest(lst[[i]])
} 
catch(stks2) <- computeCatch(stks2) 
stock(stks2) <- computeStock(stks2) 
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(FLStocks(orig=stk, sim=stks, fitsim=stks2))
@

\subsubsection{Paralell computing}

This is an example of how to use the parallel package to run assessments. In this example each iteration is a dataset, including surveys, and we'll run one assessment for each iteration. Afterwards the data is pulled back together in a FLStock object and plotted. Only 20 iterations are ran to avoid taking too long. Also note that we're using 4 cores. This parameter depends on the computer being used. These days almost all computers have 2 cores.

Finnally, compare this code with the one for replicating WCSAM and note that it's exacly the same, except that we're using mclapply, which is in parallel, instead of lapply. 

<<>>=
nits <- 20
fit <- a4aSCA(ple4, ple4.indices[1]) 
stk <- ple4 + fit
fits <- simulate(fit, nits)
stks <- ple4 + fits 
idxs <- ple4.indices[1]
index(idxs[[1]]) <- index(fits)[[1]]
library(parallel)
options(mc.cores=4)
lst <- mclapply(split(1:nits, 1:nits), function(x){
	out <- try(sca(iter(stks, x), FLIndices(iter(idxs[[1]], x)))) 
	if(is(out, "try-error")) NULL else out
})

stks2 <- stks
for(i in 1:nits){
	iter(catch.n(stks2), i) <- catch.n(lst[[i]])
	iter(stock.n(stks2), i) <- stock.n(lst[[i]])
	iter(harvest(stks2), i) <- harvest(lst[[i]])
} 
catch(stks2) <- computeCatch(stks2) 
stock(stks2) <- computeStock(stks2) 
stks3 <- FLStocks(orig=stk, sim=stks, fitsim=stks2)

@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(stks3)
@

\subsection{Model averaging}

We follow the paper Colin et al. Only the AIC averaging is implemented for now.

<<>>=
data(ple4)
data(ple4.indices)
f1 <- sca(ple4, ple4.indices, fmodel=~ factor(age) + s(year, k=20), qmodel=list(~ s(age, k = 4), ~ s(age, k = 4), ~ s(age, k = 3)), fit = "assessment")
f2 <- sca(ple4, ple4.indices, fmodel=~ factor(age) + s(year, k=20), qmodel=list(~ s(age, k = 4)+year, ~ s(age, k = 4), ~ s(age, k = 3)), fit = "assessment")
stock.sim <- ma(a4aFitSAs(list(f1=f1, f2=f2)), ple4, AIC, nsim = 100)
stks <- FLStocks(f1=ple4+f1, f2=ple4+f2, ma=stock.sim)
flqs <- lapply(stks, ssb)
flqs <- lapply(flqs, iterMedians)
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
xyplot(data~year, groups=qname, data=flqs, type="l")
@

<<figure=TRUE,fig.pos="h!",echo=FALSE>>=
plot(stks)
@

\end{document}

