\documentclass[a4paper,english,10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{color} 
\usepackage{float}
\usepackage{longtable}
\usepackage[bottom]{footmisc}
\usepackage{url}
\usepackage{natbib}
\usepackage{authblk}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{babel}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=3cm,bmargin=2cm,lmargin=2cm,rmargin=3cm}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\hypersetup{
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,        % color of internal links
    citecolor=red,         % color of links to bibliography
    filecolor=blue,        % color of file links
    urlcolor=blue          % color of external links
}

% Define some handy formatting
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\pkg}[1]{{\texttt{#1}}}
\newcommand{\class}[1]{{\textit{#1}}}
\newcommand{\R}{{\normalfont\textsf{R }}{}}
\newcommand{\args}[1]{{\texttt{#1}}}

%----------------------------------------------------------------------------------

\begin{document}
%\SweaveOpts{concordance=TRUE}

\title{Stock assessment and management advice with a4a methods \\ DRAFT}

\author[1]{Ernesto Jardim}
\author[1]{Colin Millar}
\author[1]{Finlay Scott}
\affil[1]{European Commission, Joint Research Centre, IPSC / Maritime Affairs Unit, 21027 Ispra (VA), Italy}
\affil[*]{Corresponding author \href{mailto:ernesto.jardim@jrc.ec.europa.eu}{ernesto.jardim@jrc.ec.europa.eu}}

\maketitle
\tableofcontents
\newpage

\section{Introduction}

\begin{itemize}
	\item Objectives
	\item a4a concepts
	\begin{itemize}
		\item life history considers parameters to have distributions, it's a kind of Bayesian posteriors informed estimates, but if one runs a Bayesian analysis to estimate growth parameters the posteriors can be used  
	\end{itemize}
	\item Workflow diagram
\end{itemize}


<<load_libraries, message=FALSE, warning=FALSE>>=
#==============================================================================
# Libraries and constants 
#==============================================================================
library(FLa4a)
library(XML)
library(reshape2)
data(rfLen)
data(ple4)
data(ple4.indices)
@

<<knitr_opts, echo=FALSE>>=
opts_chunk$set(dev='png', cache=TRUE, fig.align='center', background='white')

@

<<trans_funcs, message=FALSE, warning=FALSE>>=
#==============================================================================
# Some functions for transforming the data
#==============================================================================

# quant 2 quant
qt2qt <- function(object, id=5, split="-"){
	qt <- object[,id]
	levels(qt) <- unlist(lapply(strsplit(levels(qt), split=split), "[[", 2))
	as.numeric(as.character(qt))
}

# check import and massage
cim <- function(object, n, wt, hrv="missing"){
	v <- object[sample(1:nrow(object), 1),]
	c1 <- c(n[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V6)
	c2 <- c(wt[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V7)
	if(missing(hrv)){
		c1 + c2 == 2	
	} else {
		c3 <- c(hrv[as.character(v$V5),as.character(v$V1),1,as.character(v$V2)]==v$V8)
		c1 + c2 + c3 == 3	
	}
}
@

\section{Reading files and building FLR objects}

For this document we'll use the plaice in ICES area IV dataset, provided by FLR, and a length-based simulated dataset based on red fish, using Gadget (http://www.hafro.is/gadget), provided by Daniel Howell (Institute of Marine Research, Norway).

In this section we read in the Gadget data files, and transform them into FLR objects.

First we read in the files as data frames.
<<read_gadget_files>>=
#==============================================================================
# Read files
#==============================================================================

# catch
cth.orig <- read.table("data/catch.len", skip=5)

# stock
stk.orig <- read.table("data/red.len", skip=4)

# surveys
idx.orig <- read.table("data/survey.len", skip=5)
idxJmp.orig <- read.table("data/jump.survey.len", skip=5)
idxTrd.orig <- read.table("data/tend.survey.len", skip=5)

#==============================================================================
# Recode the length categories into something usable
#==============================================================================

# catch
cth.orig[,5] <- qt2qt(cth.orig)

# stock
stk.orig[,5] <- qt2qt(stk.orig)

# surveys
idx.orig[,5] <- qt2qt(idx.orig)
idxJmp.orig[,5] <- qt2qt(idxJmp.orig)
idxTrd.orig[,5] <- qt2qt(idxTrd.orig)
@

Then we reshape the data frames into six dimensional arrays.

<<cast_data>>=
#==============================================================================
# Cast the data into arrays
#==============================================================================

# catch
cth.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=cth.orig)
cth.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=cth.orig)
hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=cth.orig)

# stock
stk.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=stk.orig)
stk.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=stk.orig)

# surveys
idx.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idx.orig)
idx.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idx.orig)
idx.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idx.orig)

idxJmp.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idxJmp.orig)
idxJmp.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idxJmp.orig)
idxJmp.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idxJmp.orig)

idxTrd.n <- acast(V5~V1~1~V2~1~1, value.var="V6", data=idxTrd.orig)
idxTrd.wt <- acast(V5~V1~1~V2~1~1, value.var="V7", data=idxTrd.orig)
idxTrd.hrv <- acast(V5~V1~1~V2~1~1, value.var="V8", data=idxTrd.orig)
@

We take the arrays and make \class{FLQuant} objects from them.

<<make_FLQs>>=
#==============================================================================
# Make FLQuant objects
#==============================================================================

# catch
dnms <- dimnames(cth.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
cth.n <- FLQuant(cth.n, dimnames=dnms)
cth.wt <- FLQuant(cth.wt, dimnames=dnms)
hrv <- FLQuant(hrv, dimnames=dnms)
units(hrv) <- "f"

# stock
dnms <- dimnames(stk.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
stk.n <- FLQuant(stk.n, dimnames=dnms)
stk.wt <- FLQuant(stk.wt, dimnames=dnms)

# stock
dnms <- dimnames(idx.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idx.n <- FLQuant(idx.n, dimnames=dnms)
idx.wt <- FLQuant(idx.wt, dimnames=dnms)
idx.hrv <- FLQuant(idx.hrv, dimnames=dnms)

dnms <- dimnames(idxJmp.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idxJmp.n <- FLQuant(idxJmp.n, dimnames=dnms)
idxJmp.wt <- FLQuant(idxJmp.wt, dimnames=dnms)
idxJmp.hrv <- FLQuant(idxJmp.hrv, dimnames=dnms)

dnms <- dimnames(idxTrd.n)
names(dnms) <- names(dimnames(FLQuant()))
names(dnms)[1] <- "len"
idxTrd.n <- FLQuant(idxTrd.n, dimnames=dnms)
idxTrd.wt <- FLQuant(idxTrd.wt, dimnames=dnms)
idxTrd.hrv <- FLQuant(idxTrd.hrv, dimnames=dnms)
@

Some sanity checks to check that the resulting objects have matching dimensions.

<<dim_check>>=
#==============================================================================
# Check dims match
#==============================================================================

# catch
cim(cth.orig, cth.n, cth.wt, hrv)

# stock
cim(stk.orig, stk.n, stk.wt)

# surveys
cim(idx.orig, idx.n, idx.wt, idx.hrv)
cim(idxJmp.orig, idxJmp.n, idxJmp.wt, idxJmp.hrv)
cim(idxTrd.orig, idxTrd.n, idxTrd.wt, idxTrd.hrv)
@

%#------------------------------------------------------------------------------
%# contents
%#------------------------------------------------------------------------------

%# length-weight relationship is a bit odd ...
%xyplot(data~len|year, groups=season, data=cth.wt/cth.n, type="l")

%# length-weight relationship is a bit odd ...
%xyplot(data~len|year, groups=season, data=stk.wt, type="l")

Finally, we make FLR objects from the data.

<<make_FLR_objects>>=
#==============================================================================
# FLR objects
#==============================================================================

rfLen.stk <- FLStockLen(stock.n=stk.n, stock.wt=stk.wt, stock=quantSums(stk.wt*stk.n), catch.n=cth.n, catch.wt=cth.wt/cth.n, catch=quantSums(cth.wt), harvest=hrv)
m(rfLen.stk)[] <- 0.05
mat(rfLen.stk)[] <- m.spwn(rfLen.stk)[] <- harvest.spwn(rfLen.stk)[] <- 0
mat(rfLen.stk)[38:59,,,3:4] <- 1
  
rfTrawl.idx <- FLIndex(index=idx.n, catch.n=idx.n, catch.wt=idx.wt, sel.pattern=idx.hrv) 
effort(rfTrawl.idx)[] <- 100

rfTrawlJmp.idx <- FLIndex(index=idxJmp.n, catch.n=idxJmp.n, catch.wt=idxJmp.wt, sel.pattern=idxJmp.hrv) 
effort(rfTrawlJmp.idx)[] <- 100

rfTrawlTrd.idx <- FLIndex(index=idxTrd.n, catch.n=idxTrd.n, catch.wt=idxTrd.wt, sel.pattern=idxTrd.hrv) 
effort(rfTrawlTrd.idx)[] <- 100

#==============================================================================
# Save the objects for future use
#==============================================================================

save(rfLen.stk, rfTrawl.idx, rfTrawlJmp.idx, rfTrawlTrd.idx, file="rfLen.rdata")
@

\section{Converting length data to age}

The stock assessment framework is based on age dynamics. To use length information it must be pre-processed before used for assessment. The rationale is that the pre-processing should give the analyst the flexibility to use a range of sources of information, \emph{e.g.} literature or online databases, to grab information about the species growth and the uncertainty about the model parameters.

Within the a4a framework this is handled using the \class{a4aGr} class. In this section we introduce the \class{a4aGr} class and look at the variety of ways that parameter uncertainty can be included.

\subsection{a4aGr - The growth class}

The convertion of length data to age is performed through the use of a growth model. The implementation is done through the \class{a4aGr} class.

<<show_a4aGr>>=
showClass("a4aGr")
@

A simple construction of \class{a4aGr} objects requires the model and parameters to be provided.
Check the help file for more information.

Here we show an example using the von Bertalanffy growth model. It is possible to use other growth models.

<<a4aGr_vB_example>>=
#==============================================================================
# An example using the von Bertalanffy growth model
#==============================================================================

# Create the a4aGr object by passing in:
# the model (length ~ time)
# the inverse of the model (time ~ length)
# the parameters
vbObj <- a4aGr(grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c("cm","ano-1","ano")))
@

The predict method allows the transformation between age and lengths.

<<predict_araGr_example>>=
#==============================================================================
# Predicting ages from lengths and vice-versa
#==============================================================================

# First we check the model and its inverse
lc=20
predict(vbObj, len=lc)
predict(vbObj, t=predict(vbObj, len=lc))==lc

# Example of converting a vector of lengths or ages
predict(vbObj, len=5:10+0.5)
predict(vbObj, t=5:10+0.5)
@

\subsection{Adding parameter uncertainty with a multivariate normal distribution}

Uncertainty is introduced through the inclusion of parameter uncertainty.
There are several ways of doing this. The simplest method is to assume that the parameters are represented using a multivariate normal distribution.
The implementation for \class{a4aGr} makes use of the \code{vcov} slot to set the parameter's covariance matrix.
%This covariance matrix can have iterations (i.e. each iteration can have a different covariance matrix). CHECK
% If the parameters or the covariance matrix have iterations then the medians accross iterations are computed before simulating. Check help for \code{mvrnorm} for more information.

Here we set the variance-covariance matrix with some made up numbers.
% Can we get some estimates from the data?

<<set_vcov_example>>=
#==============================================================================
# Setting the variance-covariance matrix
#==============================================================================

# Make an empty vcov matrix
mm <- matrix(NA, ncol=3, nrow=3)
# Fill it up with made up values
diag(mm) <- c(100, 0.001,0.001)
mm[upper.tri(mm)] <- mm[lower.tri(mm)] <- c(0.1,0.1,0.0003)
# Create the a4aGr object as before but now we also include arguments for multivariate normal uncertainty
vbObj <- a4aGr(grMod=~linf*(1-exp(-k*(t-t0))), grInvMod=~t0-1/k*log(1-len/linf), params=FLPar(linf=58.5, k=0.086, t0=0.001, units=c("cm","ano-1","ano")), vcov=mm)
@

<<simulate_vcov_example>>=
#==============================================================================
# Simulating parameters using the variance-covariance matrix
#==============================================================================

# Note that the object we have just created has a single iteration of each parameter
vbObj@params
dim(vbObj@params)
# We simulate from the a4aGr object by calling mvrnorm(). Here we create 100 iterations. 
vbNorm <- mvrnorm(10000,vbObj)
# Now we have 100 iterations of each parameter, randomly sampled from the multivariate normal distribution
vbNorm@params
dim(vbNorm@params)

# We can now convert from length to ages data based on the 100 parameter iterations.
# This gives us 100 sets of ages data.
# For example, here we convert a single length vector: 
ages <- predict(vbNorm, len=5:10+0.5)
dim(ages)
# We show the first ten only as an illustration
ages[,1:10]
@

The marginal distributions can be seen in Figure~\ref{fig:plot_norm_params}.

\begin{figure}
<<example_norm_parameter_distributions>>=
par(mfrow=c(3,1))
hist(c(params(vbNorm)["linf",]), main="linf", xlab="")
hist(c(params(vbNorm)["k",]), main="k", prob=TRUE, xlab="")
#lines(x. <- seq(min(k), max(k), len=100), dnorm(x., mean(k), sd(k)))
hist(c(params(vbNorm)["t0",]), main="t0", xlab="")
@
\caption{The marginal distributions of each of the parameters from using a multivariate normal distribution.}
\label{fig:plot_norm_params}
\end{figure}

The shape of the correlation can be seen in Figure~\ref{fig:plot_norm_scatter}.

\begin{figure}
<<example_norm_parameter_scatter>>=
splom(data.frame(t(params(vbNorm)@.Data)), pch=".")
@
\caption{Scatter plot of the 10000 samples parameter from the multivariate normal distribution.}
\label{fig:plot_norm_scatter}
\end{figure}
Growth curves for the 100 iterations can be seen in Figure~\ref{fig:plot_mv_growth}.

\begin{figure}
<<example_mv_growth_curve_plot>>=
# Generating and plotting growth curves
boxplot(t(predict(vbNorm, t=0:50+0.5)))
@
\caption{Growth curves using parameters simulated from a multivariate normal distribution.}
\label{fig:plot_mv_growth}
\end{figure}

\subsection{Adding parameter uncertainty with a multivariate triangle distribution}

One alternative to using normal distributions, particularly if you don't believe in asymptotic theory, is to use triangle distributions (\url{http://en.wikipedia.org/wiki/Triangle_distribution}). These distributions are parametrized using the minimum, maximum and median values.
This can be very attractive if the analyst needs to scrape information from the web or literature and perform some kind of meta-analysis.

<<tri_example>>=
#==============================================================================
# Example of setting a triangle distribution with values taken from FishBase
#==============================================================================

# The web address for the growth parameters for redfish (Sebastes norvegicus)
addr <- "http://www.fishbase.org/PopDyn/PopGrowthList.php?ID=501"
# Scrape the data
tab <- try(readHTMLTable(addr))
# Interrogate the data table and get vectors of the values
linf <- as.numeric(as.character(tab$dataTable[,2]))
k <- as.numeric(as.character(tab$dataTable[,4]))
t0 <- as.numeric(as.character(tab$dataTable[,5]))
# Set the min, max and median values for each parameters as a list of lists
# Note that t0 has no 'c' (max) value. This makes the distribution symmetrical
triPars <- list(list(a=min(linf), b=max(linf), c=median(linf)),
             list(a=min(k), b=max(k), c=median(k)),
             list(a=median(t0, na.rm=T)-IQR(t0, na.rm=T)/2, b=median(t0, na.rm=T)+IQR(t0, na.rm=T)/2))
# Simulate 10000 times using mvrtriangle
vbTri <- mvrtriangle(10000, vbObj, paramMargins=triPars)
@

The marginals will reflect the uncertainty on the parameter values that were scraped from FishBase but, as we don't really believe the parameters are multivariate normal we adopted a more relaxed distribution based on a \emph{t} copula with triangle marginals.
The marginal distributions can be seen in Figure~\ref{fig:plot_tri_params}.

\begin{figure}
<<example_tri_parameter_distributions>>=
par(mfrow=c(3,1))
hist(c(params(vbTri)["linf",]), main="linf", xlab="")
hist(c(params(vbTri)["k",]), main="k", prob=TRUE, xlab="")
#lines(x. <- seq(min(k), max(k), len=100), dnorm(x., mean(k), sd(k)))
hist(c(params(vbTri)["t0",]), main="t0", xlab="")
@
\caption{The marginal distributions of each of the parameters from using a multivariate triangle distribution.}
\label{fig:plot_tri_params}
\end{figure}

The shape of the correlation can be seen in Figure~\ref{fig:plot_tri_scatter}.

\begin{figure}
<<example_tri_parameter_scatter>>=
splom(data.frame(t(params(vbTri)@.Data)), pch=".")
@
\caption{Scatter plot of the 10000 samples parameter from the multivariate triangle distribution.}
\label{fig:plot_tri_scatter}
\end{figure}

Off course we can still use \code{predict()} to get see the growth model uncertainty (Figure~\ref{fig:plot_tri_growth}).

\begin{figure}
<<example_tri_parameter_growth>>=
boxplot(t(predict(vbTri, t=0:20+0.5)))
@
\caption{Growth curves using parameters simulated from a multivariate triangle distribution.}
\label{fig:plot_tri_growth}
\end{figure}

Remember that the above examples use a variance-covariance matrix that we made up.
If you want to be really geeky, you can scrape the entire growth parameters dataset from FishBase and compute the shape of the variance covariance matrix yourself.

\subsection{Adding parameter uncertainty with copulas}

A more general approach to adding parameter uncertainty is to make use of whatever copula and marginal distribution you want.
This is possible with \code{mvrcop()} function. The example below keeps the same parameters and changes only the copula type and family but a lot more can be done. Check the package \emph{copula} for more. 

% Needs more explanation

<<copula_triangle_example>>=
vbCop <- mvrcop(10000, vbObj, copula="archmCopula", family="clayton", param=2, margins="triangle", paramMargins=triPars)
@

The shape of the correlation changes (Figure~\ref{fig:plot_cop_tri_scatter}).

\begin{figure}
<<example_cop_tri_scatter>>=
splom(data.frame(t(params(vbCop)@.Data)), pch=".")
@
\caption{Scatter plot of the 10000 samples parameter from the using a triangle copula.}
\label{fig:plot_cop_tri_scatter}
\end{figure}

As well as the resulting growth curves (Figure~\ref{fig:plot_cop_tri_growth}). 

\begin{figure}
<<example_cop_tri_growth>>=
boxplot(t(predict(vbCop, t=0:20+0.5)))
@
\caption{Growth curves from the using a triangle copula.}
\label{fig:plot_cop_tri_growth}
\end{figure}

\subsection{The "l2a" method}

After introducing uncertainty on the growth model through the parameters it's time to transform the length-based dataset into an age-based dataset. The method that deals with this process is \code{l2a()}. The implementation of this method for the \class{FLQuant} class is the main workhorse. There's two other implementations, for the \class{FLStock} and \class{FLIndex} classes, which are mainly wrappers that call the \class{FLQuant} method several times.

When converting from length-based data to age-based data you need to be aware of how the aggregation of length classes is performed.
For example, individuals in length classes, 1-2, 2-3, and 3-4 cm may all be considered as being of age 1 (obviously depending on the growth model). How should the values in those length classes be combined?

If the values are abundances then the values should be summed.
Summing other types of values such as weights and rates (like fishing mortality) does not make sense. Instead these values shoud be averaged over the length classes.
This is controlled using the \code{stat} argument which can be either \code{mean} or \code{sum} (the default).

%Note that, for the moment, this method is quite slow. There's a double loop in the code that makes it slow, but we're working on a better solution.

<<FLQ_l2a>>=
#==============================================================================
# Convert length-based FLQuant to age-based FLQuant
#==============================================================================

# We demontrate the method by converting a catch-at-length FLQuant to a catch-at-age FLQuant.
# First we make the an a4aGr object with a multivariate triangle distribution.
# We use 10 iterations as an example. More would be better.
vbTriSmall <- mvrtriangle(10, vbObj, paramMargins=triPars)
# We use l2a by passing in the length-based FLQuant and the a4aGr object
cth.n <- l2a(catch.n(rfLen.stk), vbTriSmall)
@

The results can be seen in Figure~\ref{fig:plot_flq_l2a}.

\begin{figure}
<<example_plot_flq_l2a>>=
quant(cth.n) <- "len"
xyplot(data~len|qname+season, groups=year, data=(FLQuants(len=catch.n(rfLen.stk), age=cth.n)), type="l", xlab="", ylab="numbers")
@
\caption{Length and age-based FLQuant.}
\label{fig:plot_flq_l2a}
\end{figure}

Instead of converting one \class{FLQuant} at a time, we can convert entire \class{FLStock} and \class{FLIndex} objects.
In these cases the individual \class{FLQuant} slots of those classes are converted from length-based to age-based.
As mentioned above, the aggregation method depends on the type of values the slots contain.
The abundance slots (\code{*.n}, such as \code{stock.n}) are summed.
The \code{*.wt}, \code{m}, \code{mat}, \code{harvest.spwn}, \code{m.spwn} and \code{harvest} slots of an \class{FLStock} object are averaged. 
The weights are then corrected to ensure that the total catch weight (and landings, discards and stock weights) in the age-based object is the same as that in the length-based object.
The \code{index}, \code{catch.wt}, \code{index.var}, \code{sel.pattern} and \code{index.q} slots of an \class{FLIndex} object are averaged.

<<FLS_FLI_l2a>>=
#==============================================================================
# Convert length-based FLStock and FLIndec to age-based 
#==============================================================================

aStk <- l2a(rfLen.stk, vbTriSmall)
aIdx <- l2a(rfTrawl.idx, vbTriSmall)
@

When converting with \code{l2a()} there are a number of defaults that of which the user should be aware. 
%
% Ages contiguous but need trimming to something sensible
%
All lengths above Linf are converted to the maximum age. This is not true in most cases, but that's as far as one can go with a age length growth model. There is no information on the model to deal with individuals larger than the maximum length. The variability around Linf is dealt by the randamization of the parameter Linf, and the cappacity to withold all the data depends on how well the analysist matches the variance of the parameter with the variance on the data.

\section{Dealing with natural mortality}

Natural mortality is dealt as an external parameter to the stock assessment model. The rationale is similar to that of growth. One should be able to grab information from whichever sources are available and use that information in a way that it propagates into stock assessment.

The mechanism used by a4a is to build an interface that makes it transparent, flexible and hopefully easy to explore different options. In relation to natural mortality it means that the analyst should be able to use distinct models like Gislasson's, Charnov's, Pauly's, etc in a coherent framework making it possible to compare the outcomes of the assessment. 

The smoother way to insert natural mortality in stock assessment is to use an \emph{a4aM} object and run the method \emph{m} to compute the values. The output is a \emph{FLQuant} that should be directly inserted in the \emph{FLStock} object to be used for assessment.   

\subsection{a4aM - The M class}

Natural mortality is implemented in a class named \emph{a4aM} which has three models of the class \emph{FLModelSim}. Each model represents one effects. An age effect, an year effect and a time trend, named \emph{shape}, \emph{level} and \emph{trend}, respectively. Check the help files for more information.

<<>>=
showClass("a4aM")
@

A simple construction of \emph{a4aM} objects requires the models and parameters to be provided. The default method will build each of these models as a constant value of 1. For example the usual "0.2" guessestimate could be set up by  

<<>>=
mod2 <- FLModelSim(model=~a, params=FLPar(a=0.2))
m1 <- a4aM(level=mod2)
@

Off course that would be too much work for the outcome. The interest is in using more knowledge setting M. The following example uses Jensen's second estimator (Kenshington, 2013) $M=1.5K$ and an exponential decay to set up the level and shape of M.

<<>>=
#--------------------------------------------------------------------
# models or shape and level
#--------------------------------------------------------------------
mod1 <- FLModelSim(model=~exp(-age-0.5))
mod2 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
#--------------------------------------------------------------------
# constructor
#--------------------------------------------------------------------
m2 <- a4aM(shape=mod1, level=mod2)
@ 

In alternative, an external factor may have impact on natural mortality which can be added through the \emph{trend} model. Suppose M depends on NAO through some mechanism that results in having lower M when NAO is negative and higher when it's positive. The impact is represented by the NAO value on the quarter before spawning, which occurs in the second quarter. 

<<>>=
#--------------------------------------------------------------------
# get NAO
#--------------------------------------------------------------------
nao.orig <- read.table("http://www.cdc.noaa.gov/data/correlation/nao.data", skip=1, nrow=62, na.strings="-99.90")
dnms <- list(quant="nao", year=1948:2009, unit="unique", season=1:12, area="unique")
nao.flq <- FLQuant(unlist(nao.orig[,-1]), dimnames=dnms, units="nao")
# build covar
nao <- seasonMeans(nao.flq[,,,1:3]) 
nao <- nao>0
#--------------------------------------------------------------------
# the trend model M increases 50% if NAO is positive on the first quarter
#--------------------------------------------------------------------
mod3 <- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5))
#--------------------------------------------------------------------
# constructor
#--------------------------------------------------------------------
mod1 <- FLModelSim(model=~exp(-age-0.5))
mod2 <- FLModelSim(model=~1.5*k, params=FLPar(k=0.4))
m3 <- a4aM(shape=mod1, level=mod2, trend=mod3)
@

\subsection{Adding multivariate normal parameter uncertainty}

Uncertainty is added through error on parameters. In the case of this class it makes use of the \emph{FLModelSim} "mvr" methods. A wrapper for \emph{mvrnorm} was implemented, but all the other options must be carried out in each sub-model at the time.

<<>>=
#--------------------------------------------------------------------
# the same exponential decay for shape
#--------------------------------------------------------------------
mod1 <- FLModelSim(model=~exp(-age-0.5))
#--------------------------------------------------------------------
# For level we'll use Jensen's third estimator (Kenshington, 2013).
#--------------------------------------------------------------------
mod2 <- FLModelSim(model=~k^0.66*t^0.57, params=FLPar(matrix(c(0.4,10)), dimnames=list(params=c("k","t"), iter=1)), vcov=array(c(0.002, 0.01,0.01, 1), dim=c(2,2)))
#--------------------------------------------------------------------
# and a trend from NAO
#--------------------------------------------------------------------
mod3 <- FLModelSim(model=~1+b*nao, params=FLPar(b=0.5), vcov=matrix(0.02))
#--------------------------------------------------------------------
# create object and simulate
#--------------------------------------------------------------------
m4 <- a4aM(shape=mod1, level=mod2, trend=mod3)
m4 <- mvrnorm(100, m4)
@ 

In this particular case, the \emph{shape} model will not be randomized because it doesn't have a variance covariance matrix. Also note that because there is only one parameter in the \emph{trend} model, the randomization will use a univariate normal distribution. The same could be achieved with

<<>>=
m4 <- a4aM(shape=mod1, level=mvrnorm(100, mod2), trend=mvrnorm(100, mod3))
@

Note: How to include ageing error ???

\subsection{Adding parameter uncertainty with copulas}

As stated above these processes make use of the methods implemented for \emph{FLModelSim}. EXPAND... In the following example we'll use Gislason's second estimator (REF), $M_l=K(\frac{L_inf}{l})^1.5$.

<<>>=
linf <- 60
k <- 0.4
# vcov matrix
mm <- matrix(NA, ncol=2, nrow=2)
# 10% cv
diag(mm) <- c((linf*0.1)^2, (k*0.1)^2)
# 0.2 correlation
mm[upper.tri(mm)] <- mm[lower.tri(mm)] <- c(0.05)
# a good way to check is using cov2cor
cov2cor(mm)
# create object
mgis2 <- FLModelSim(model=~K*(linf/len)^1.5, params=FLPar(linf=linf, K=k), vcov=mm)

pars <- list(list(55,65), list(a=0.3, b=0.6, c=0.35))
mgis2 <- mvrtriangle(1000, mgis2, paramMargins=pars)
@

<<fig=TRUE>>=
splom(t(params(mgis2)@.Data))
@

<<fig=TRUE>>=
par(mfrow=c(2,1))
hist(c(params(mgis2)["linf",]), main="Linf")
hist(c(params(mgis2)["K",]), main="K")
@

Use the constructor or the set method to add the new model. Note that we have a quite complex method now for \emph{M}. A length based \emph{shape} model from Gislason's work, Pauly's based temperature \emph{level} and a time trend depending on NAO. 

<<>>=
m5 <- a4aM(shape=mgis2, level=mod2, trend=mod3)
# or
m5 <- m4
level(m5) <- mgis2
@

\subsection{The "m" method}

The \emph{m} method is the workhorse on computing natural mortality. The method returns a \emph{FLQuant} that can be inserted in an \emph{FLStock} for posterior usage by the assessment method. Note that if the models use \emph{age} and/or \emph{year} as terms, the method expects these to be included in the call (will be passed through the \ldots argument). If they're not, the method will use the range slot to work out the ages and/or years that should be predicted. If \emph{age} and/or \emph{year} are not model terms, the method will use the range slot to define the dimensions of the resulting \emph{M} \emph{FLQuant}.

<<>>=
# simple
m(m1)
# with ages
rngage(m1) <- c(0,15)
m(m1)
# with ages and years
rngyear(m1) <- c(2000, 2010)
m(m1)
@

The next example as aage based shape. The information on the range of ages can be passed when calling \emph{m}, or else the method will pick it up from the \emph{range} slot. Note that in this case \emph{mbar} becames relevant. It's the range of ages that is used to compute the mean level, which will match the \emph{level} model.

<<>>=
# simple
m(m2)
# with ages
rngage(m2) <- c(0,15)
m(m2)
# with ages and years
rngyear(m2) <- c(2000, 2003)
m(m2)
# note that 
predict(level(m2))
# is similar to 
m(m2)["0"]
# that's because mbar is "0"
rngmbar(m2)
# changing ...
rngmbar(m2)<- c(0,5)
quantMeans(m(m2)[as.character(0:5)])
@


<<>>=
# simple
m(m3, nao=1)
# with ages
rngage(m3) <- c(0,15)
m(m3, nao=0)
# with ages and years
rngyear(m3) <- c(2000, 2003)
m(m3, nao=as.numeric(nao[,as.character(2000:2003)]))
@

<<>>=
# simple
m(m4, nao=1)
# with ages
rngage(m4) <- c(0,15)
m(m4, nao=0)
# with ages and years
rngyear(m4) <- c(2000, 2003)
m(m4, nao=as.numeric(nao[,as.character(2000:2003)]))
@

<<fig=TRUE>>=
bwplot(data~factor(quant)|year, data=m(m4, nao=as.numeric(nao[,as.character(2000:2003)])))
@

or this!
<<fig=TRUE>>=
plotIters(m(m4, nao=as.numeric(nao[,as.character(2000:2003)])), by = "year")
@


\section{Running assessments}

There are two basic types of assessments available from using \texttt{a4a}: the management procedure (MP) fit and the full assessment fit.  The MP fit does not compute estimates of covariances and is therefore quicker to execute, while the full assessment fit returns parameter estimates and their covariances and hence retains the ability to simulate from the model at the expense of longer fitting time.


\subsection{a4aFit* - The fit classes}

The basic model output is contained in the \texttt{a4aFit} class.  This object contains only the fitted values.

<<>>=
showClass("a4aFit")
@

Fitted values are stored in the \texttt{stock.n}, \texttt{harvest}, \texttt{catch.n} and \texttt{index} slots.  It also contains information carried over from the stock object used to fit the model: the name of the stock in \texttt{name}, any description provided in \texttt{desc} and the age and year range and mean F range in \texttt{range}.  There is also a wall clock that has a breakdown of the time taken o run the model.

The full assessment fit returns an object of \texttt{a4aFitSA} class:

<<>>=
showClass("a4aFitSA")
@

The additional slots in the assessment output is the \texttt{fitSumm} and \texttt{pars} slots which are containers for model summaries and the model parameters.  The \texttt{pars} slot is a class of type \texttt{SCAPars} which is itself composed of sub-classes, designed to contain the information necessary to simulate from the model.

<<>>=
showClass("SCAPars")
showClass("a4aStkParams")
@

for example, all the parameters required so simulate a time-series of mean F trends is contained in the \texttt{stkmodel} slot, which is a class of type \texttt{a4aStkParams}.  This class contains the relevant submodels (see later), their parameters \texttt{params} and the joint covariance matrix \texttt{vcov} for all stock related parameters.

\subsection{The submodels}

In the \texttt{a4a} assessment model, the model structure is defined by submodels.  These are models for the different parts of a statistical catch at age model that requires structural assumptions, such as the selectivity of the fishing fleet, or how F-at-age changes over time.  It is advantageous to write the model for F-at-age and survey catchability as linear models (by working with log F and log Q) becuase it allows us to use the linear modelling tools available in R:  see for example gam formulas, or factorial design formulas using lm.  In R's linear modelling lanquage, a constant model is coded as $\sim$ 1, while a slope over age would simply be $\sim$ age.  Extending this we can write a traditional year / age seperable F model like $\sim$ factor(age) $+$ factor(year).

There are effectively 5 submodels in operation: the model for F-at-age, a model for initial age structure, a model for recruitment, a (list) of model(s) for survey catchability-at-age, and a list of models for the observation variance of catch.n and the survey indices.  In practice, we fix the variance models and the initial age structure models, but in theory these can be changed.  A basic set of submodels would be

<<>>=
fmodel <- ~ factor(age) + factor(year)
qmodel <- list(~ factor(age)) 
@

\subsection{Run !!}

running the model is done by

<<>>=
fit <- a4a(fmodel, qmodel, stock = ple4, indices = ple4.indices[1])
@

note that because the survey index for plaice has missing values we get a warning saying that we assume these values are missing at random, and not because the observations were zero.

We can inspect the summaries from this fit my adding it to the origional stock object, for example to see the fitted fbar we can do

<<figure=TRUE>>=
fitstk <- ple4 + fit
plotIters(fbar(fitstk))
@


\subsection{Some more examples}

We will now take a look at some examples for F models and the forms that we can get.  Lets start with a separable model in which we model selectivity at age as an (unpenalised) thin plate spline.  We will use the North Sea Plaice data again, and since this has 10 ages we will use a simple rule of thumb that the spline should have fewer than $\frac{10}{2} = 5$ degrees of freedom, and so we opt for 4 degrees of freedom.  We will also do the same for year and model the change in F through time as a smoother with 20 degrees of freedom.

<<figure=TRUE>>=
fmodel <- ~ s(age, k=4) + s(year, k = 20)
qmodel <- list( ~ factor(age))
fit1 <- a4a(fmodel, qmodel, stock = ple4, indices = ple4.indices[1]) 
wireframe(data ~ year + age, data = as.data.frame(harvest(fit1)), drape = TRUE)
@

Lets now investigate some variations in the selectivity shape with time, but only a little... we can do this by adding a smooth interaction term in the fmodel

<<figure=TRUE>>=
fmodel <- ~ s(age, k=4) + s(year, k = 20) + te(age, year, k = c(3,3))
qmodel <- list( ~ factor(age))
fit2 <- a4a(fmodel, qmodel, stock = ple4, indices = ple4.indices[1]) 
wireframe(data ~ year + age, data = as.data.frame(harvest(fit2)), drape = TRUE)
@

A further move is to free up the Fs to vary more over time

<<figure=TRUE>>=
fmodel <- ~ te(age, year, k = c(4,20))
qmodel <- list( ~ factor(age))
fit2 <- a4a(fmodel, qmodel, stock = ple4, indices = ple4.indices[1]) 
wireframe(data ~ year + age, data = as.data.frame(harvest(fit2)), drape = TRUE)
@

In the last examples the Fs are linked across age and time.  What if we want to free up a specific age class because in the residuals we see a consistent pattern.  This can happen, for example, if the spatial distribution of juvenilles is disconnected to the distribution of adults.  The fishery focuses on the adult fish, and therefore the the F on young fish is a function of the distribution of the juveniles and could deserve a seperate model.  This can be achieved by

<<figure=TRUE>>=
fmodel <- ~ te(age, year, k = c(4,20)) + s(year, k = 5, by = as.numeric(age==1))
qmodel <- list( ~ factor(age))
fit3 <- a4a(fmodel, qmodel, stock = ple4, indices = ple4.indices[1]) 
wireframe(data ~ year + age, data = as.data.frame(harvest(fit3)), drape = TRUE)
@

Please note that each of these model \emph{structures} lets say, have not been tuned to the data.  The degrees of freedom of each model can be better tuned to the data by using model selection procedures such as AIC or BIC.



\end{document}

